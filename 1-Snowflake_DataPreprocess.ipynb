{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "SimpleCSVAnalyzerSnowflake: HDB Resale CSV Analyzer for Snowflake Notebooks\n",
    "===============================================================================\n",
    "\n",
    "This class performs a full ETL (Extract, Transform, Load) process on HDB resale\n",
    "flat data stored as CSV files in a Snowflake stage. It automates data loading,\n",
    "cleaning, transformation, validation, and saving to a Snowflake table.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "| Step | Description                                                         |\n",
    "|------|----------------------------------------------------------------------|\n",
    "| 1    | Load CSV files from a Snowflake stage into pandas DataFrames        |\n",
    "| 2    | Derive AGE column from REMAINING_LEASE or LEASE_COMMENCE_DATE        |\n",
    "| 3    | Clean text columns (uppercase conversion and NaN handling)           |\n",
    "| 4    | Filter unnecessary columns and ensure column consistency             |\n",
    "| 5    | Combine all cleaned CSV files into a single DataFrame               |\n",
    "| 6    | Split 'MONTH' column into 'YEAR' and 'MONTH_NUM'                    |\n",
    "| 7    | Display a sample of the cleaned data                                 |\n",
    "| 8    | Check for and remove duplicate rows                                  |\n",
    "| 9    | Save the final cleaned dataset to a Snowflake table (default: HDB_SILVER) |\n",
    "| 10   | Query and display sample rows from the saved Snowflake table        |\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Usage:\n",
    "    analyzer = SimpleCSVAnalyzerSnowflake()\n",
    "    analyzer.run_preprocess()\n",
    "\n",
    "Dependencies:\n",
    "    - snowflake.snowpark\n",
    "    - pandas\n",
    "    - datetime\n",
    "\n",
    "Assumptions:\n",
    "    - Snowflake session is active and authenticated\n",
    "    - CSV files are stored in the provided Snowflake stage path\n",
    "    - Files contain consistent HDB resale data format\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "class SimpleCSVAnalyzerSnowflake:\n",
    "    \"\"\"Complete CSV Analyzer for HDB resale flat data in Snowflake Notebook\"\"\"\n",
    "\n",
    "    def __init__(self, stage=\"@HDB_stage/Resale\"):\n",
    "        self.session = get_active_session()\n",
    "        self.stage = stage\n",
    "        self.csv_files = {}\n",
    "        self.final_data = None\n",
    "\n",
    "    # Step 1: Load CSVs from stage\n",
    "    def load_all_csv_files(self):\n",
    "        print(\"Step 1: Loading ResaleFlat CSV files from Snowflake stage...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        file_list = self.session.sql(f\"LIST {self.stage}\").collect()\n",
    "        resale_flat_files = [\n",
    "            row[\"name\"] for row in file_list\n",
    "            if row[\"name\"].upper().endswith(\".CSV\") and \"RESALE\" in row[\"name\"].upper()\n",
    "        ]\n",
    "\n",
    "        if not resale_flat_files:\n",
    "            print(\"‚ùå No CSV files starting with 'Resale' found in stage!\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Found {len(resale_flat_files)} Resale CSV files:\")\n",
    "        for i, file in enumerate(resale_flat_files, 1):\n",
    "            print(f\"   {i}. {file}\")\n",
    "\n",
    "        for file_path in resale_flat_files:\n",
    "            try:\n",
    "                stage_path = f\"@{file_path}\"\n",
    "                df = self.session.read.options({\n",
    "                    \"FIELD_OPTIONALLY_ENCLOSED_BY\": '\"',\n",
    "                    \"SKIP_HEADER\": 0,\n",
    "                    \"HEADER\": True      # use first row as header\n",
    "                }).csv(stage_path)\n",
    "\n",
    "                pdf = df.to_pandas()\n",
    "                pdf.columns = [col.strip().upper() for col in pdf.columns]\n",
    "                print(\"Columns in this CSV:\", list(pdf.columns))\n",
    "                self.csv_files[file_path] = pdf\n",
    "                print(f\"‚úÖ Loaded: {file_path} | Rows: {len(pdf):,} | Cols: {len(pdf.columns)}\")\n",
    "                \n",
    "            except Exception as e: \n",
    "                print(f\"‚ùå Failed to load {file_path}: {e}\")\n",
    "\n",
    "        return self.csv_files\n",
    "\n",
    "    # Step 2: Fix AGE columns\n",
    "    def fix_age_columns(self):\n",
    "        \"\"\"Step 2: Fix AGE columns to make all files compatible\"\"\"\n",
    "        print(\"\\nStep 2: Fixing AGE columns...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        current_year = datetime.now().year\n",
    "        print(f\"Using current year: {current_year}\")\n",
    "        \n",
    "        for file_name, data in self.csv_files.items():\n",
    "            print(f\"\\nüîß Processing: {file_name}\")\n",
    "            \n",
    "            # Check what columns exist - convert to lowercase for comparison\n",
    "            column_names = [col.lower() for col in data.columns]\n",
    "            \n",
    "            # If 'remaining_lease' exists, convert to AGE = 99 - lease_years\n",
    "            if 'remaining_lease' in column_names:\n",
    "                old_col_name = None\n",
    "                for col in data.columns:\n",
    "                    if col.lower() == 'remaining_lease':\n",
    "                        old_col_name = col\n",
    "                        break\n",
    "\n",
    "                # Parse remaining lease text to extract years\n",
    "                def parse_remaining_lease(lease_str):\n",
    "                    if pd.isna(lease_str) or lease_str == '':\n",
    "                        return 0\n",
    "                    lease_str = str(lease_str).strip()\n",
    "\n",
    "                    # Try to extract years from formats like \"61 years 04 months\"\n",
    "                    import re\n",
    "                    year_match = re.search(r'(\\d+)\\s*years?', lease_str.lower())\n",
    "                    if year_match:\n",
    "                        return int(year_match.group(1))\n",
    "\n",
    "                    # If it's already a number, use it\n",
    "                    try:\n",
    "                        return int(float(lease_str))\n",
    "                    except:\n",
    "                        return 0\n",
    "\n",
    "                # Apply the parsing function and compute AGE\n",
    "                data['AGE'] = data[old_col_name].apply(lambda x: 99 - parse_remaining_lease(x))\n",
    "                print(f\"   ‚úÖ Converted '{old_col_name}' to AGE = 99 - remaining_lease\")\n",
    "                print(f\"   Sample values: {data['AGE'].head(3).tolist()}\")\n",
    "\n",
    "                \n",
    "            # If no 'remaining_lease' but 'lease_commence_date' exists, create AGE\n",
    "            elif 'lease_commence_date' in column_names:\n",
    "                lease_col = None\n",
    "                for col in data.columns:\n",
    "                    if col.lower() == 'lease_commence_date':\n",
    "                        lease_col = col\n",
    "                        break\n",
    "                \n",
    "                # Convert to numeric and calculate AGE\n",
    "                data[lease_col] = pd.to_numeric(data[lease_col], errors='coerce')\n",
    "                data['AGE'] = current_year - data[lease_col]\n",
    "                print(f\"   ‚úÖ Created AGE column from '{lease_col}'\")\n",
    "                print(f\"   Formula: AGE = {current_year} - {lease_col}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è No lease columns found in {file_name}\")\n",
    "\n",
    "\n",
    "    # Step 3: Clean text columns\n",
    "    def clean_text_columns(self):\n",
    "        print(\"\\nStep 3: Converting text columns to uppercase...\")\n",
    "        print(\"=\" * 50)\n",
    "        text_columns = ['TOWN', 'FLAT_TYPE', 'STREET_NAME', 'FLAT_MODEL']\n",
    "        for file_name, data in self.csv_files.items():\n",
    "            for col in text_columns:\n",
    "                if col in data.columns:\n",
    "                    data[col] = data[col].astype(str).str.upper()\n",
    "                    data[col] = data[col].replace('NAN', pd.NA)\n",
    "                    print(f\"   ‚úÖ Converted {col} to uppercase in {file_name}\")\n",
    "\n",
    "    # Step 4: Filter dataset and check column consistency\n",
    "    def prepare_before_combining(self):\n",
    "        print(\"\\nStep 4: Preparing dataset before combining...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if not self.csv_files:\n",
    "            print(\"‚ùå No CSV files loaded\")\n",
    "            return False\n",
    "\n",
    "        # Filter columns: remove unnecessary ones before combining\n",
    "        exclude_cols = ['STREET_NAME', 'SOURCE_FILE', 'LEASE_COMMENCE_DATE','REMAINING_LEASE','REMAINING_LEASE_YEARS','BLOCK']\n",
    "        for file_name, data in self.csv_files.items():\n",
    "            keep_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "            self.csv_files[file_name] = data[keep_cols].copy()\n",
    "            print(f\"   ‚úÖ Columns filtered for {file_name}: {len(self.csv_files[file_name].columns)} columns kept\")\n",
    "\n",
    "        # Check column consistency\n",
    "        col_sets = [sorted(list(df.columns)) for df in self.csv_files.values()]\n",
    "        first_cols = col_sets[0]\n",
    "        all_match = all(cols == first_cols for cols in col_sets)\n",
    "        if not all_match:\n",
    "            print(\"‚ùå Column mismatch detected between CSV files!\")\n",
    "            for i, cols in enumerate(col_sets):\n",
    "                print(f\"   File {i+1} columns: {cols}\")\n",
    "            return False\n",
    "        print(\"‚úÖ All CSV files have consistent columns.\")\n",
    "        return True\n",
    "\n",
    "    # Step 5: Combine all CSVs into one DataFrame\n",
    "    def combine_all_files(self):\n",
    "        print(\"\\nStep 5: Combining all files...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        if not self.prepare_before_combining():\n",
    "            print(\"‚ùå Cannot combine CSVs due to column mismatch\")\n",
    "            return None\n",
    "\n",
    "        dfs = []\n",
    "        for file_name, data in self.csv_files.items():\n",
    "            data['SOURCE_FILE'] = file_name\n",
    "            dfs.append(data)\n",
    "\n",
    "        if dfs:\n",
    "            self.final_data = pd.concat(dfs, ignore_index=True)\n",
    "            # Convert numeric columns\n",
    "            for col in ['RESALE_PRICE', 'FLOOR_AREA_SQM']:\n",
    "                if col in self.final_data.columns:\n",
    "                    self.final_data[col] = self.final_data[col].fillna(0).astype(int)\n",
    "            print(f\"‚úÖ Combined successfully! Rows: {len(self.final_data):,}, Cols: {len(self.final_data.columns)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No files to combine\")\n",
    "\n",
    "    # Step 6: Split MONTH field\n",
    "    def split_month_field(self):\n",
    "        print(\"\\nStep 6: Splitting MONTH field...\")\n",
    "        print(\"=\" * 50)\n",
    "        if self.final_data is None:\n",
    "            print(\"‚ùå No data available\")\n",
    "            return\n",
    "        if 'MONTH' not in self.final_data.columns:\n",
    "            print(\"‚ùå MONTH column not found\")\n",
    "            return\n",
    "\n",
    "        self.final_data['MONTH'] = pd.to_datetime(self.final_data['MONTH'], errors='coerce')\n",
    "        self.final_data['YEAR'] = self.final_data['MONTH'].dt.year\n",
    "        self.final_data['MONTH_NUM'] = self.final_data['MONTH'].dt.month\n",
    "        print(\"‚úÖ Split MONTH into YEAR and MONTH_NUM\")\n",
    "\n",
    "    # Step 7: Show sample data\n",
    "    def show_sample_data(self, n=3):\n",
    "        print(\"\\nStep 7: Sample of filtered data...\")\n",
    "        print(\"=\" * 50)\n",
    "        if self.final_data is not None:\n",
    "            print(self.final_data.head(n))\n",
    "            print(\"\\nColumns:\", list(self.final_data.columns))\n",
    "\n",
    "    # Step 8: Check duplicates\n",
    "    def check_duplicate_data(self):\n",
    "        print(\"\\nStep 8: Checking for duplicate rows...\")\n",
    "        print(\"=\" * 50)\n",
    "        if self.final_data is None:\n",
    "            print(\"‚ùå No data available\")\n",
    "            return\n",
    "\n",
    "        duplicate_rows = self.final_data[self.final_data.duplicated()]\n",
    "        num_duplicates = len(duplicate_rows)\n",
    "        if num_duplicates > 0:\n",
    "            print(f\"üîç Found {num_duplicates:,} duplicate rows. Dropping duplicates...\")\n",
    "            self.final_data = self.final_data.drop_duplicates()\n",
    "            print(f\"‚úÖ Remaining rows: {len(self.final_data):,}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No duplicate rows found\")\n",
    "\n",
    "    # Step 9: Save final DataFrame to Snowflake table\n",
    "    def save_to_snowflake_table(self, table_name=\"HDB_SILVER\"):\n",
    "        print(\"\\nStep 9: Saving final dataset to Snowflake table...\")\n",
    "        print(\"=\" * 50)\n",
    "        if self.final_data is None:\n",
    "            print(\"‚ùå No data available to save\")\n",
    "            return\n",
    "\n",
    "        # Exclude MONTH and SOURCE_FILE columns\n",
    "        columns_to_drop = []\n",
    "        if 'MONTH' in self.final_data.columns:\n",
    "            columns_to_drop.append('MONTH')\n",
    "        if 'SOURCE_FILE' in self.final_data.columns:\n",
    "            columns_to_drop.append('SOURCE_FILE')\n",
    "            \n",
    "        df_to_save = self.final_data.drop(columns=columns_to_drop) if columns_to_drop else self.final_data.copy()\n",
    "        \n",
    "        # Reset index to fix the pandas index warning\n",
    "        df_to_save = df_to_save.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Final columns to save: {list(df_to_save.columns)}\")\n",
    "        print(f\"Rows to save: {len(df_to_save):,}\")\n",
    "\n",
    "        try:\n",
    "            # Write to Snowflake (replace table if exists)\n",
    "            self.session.write_pandas(\n",
    "                df_to_save,\n",
    "                table_name,\n",
    "                auto_create_table=True,\n",
    "                overwrite=True\n",
    "            )\n",
    "            print(f\"‚úÖ Saved final dataset to Snowflake table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save table {table_name}: {e}\")\n",
    "\n",
    "    # Step 10: Query and show top 3 rows from Snowflake table\n",
    "    def show_snowflake_table_sample(self, table_name=\"HDB_SILVER\", n=3):\n",
    "        print(f\"\\nStep 10: Showing top {n} rows from Snowflake table {table_name}...\")\n",
    "        print(\"=\" * 70)\n",
    "        try:\n",
    "            # Query the table from Snowflake\n",
    "            result = self.session.sql(f\"SELECT * FROM {table_name} LIMIT {n}\").collect()\n",
    "            \n",
    "            if result:\n",
    "                print(f\"‚úÖ Top {n} rows from {table_name}:\")\n",
    "                print(\"-\" * 100)\n",
    "                for i, row in enumerate(result, 1):\n",
    "                    print(f\"Row {i}: {dict(row.asDict())}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                # Also show column names\n",
    "                columns_result = self.session.sql(f\"DESCRIBE TABLE {table_name}\").collect()\n",
    "                column_names = [row['name'] for row in columns_result]\n",
    "                print(f\"\\nTable columns ({len(column_names)}): {column_names}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No data found in table {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to query table {table_name}: {e}\")\n",
    "        \n",
    "    # Main execution method\n",
    "    def run_preprocess(self):\n",
    "        print(\"üöÄ Starting preprocess, load and clean HDB data\")\n",
    "        print(\"=\" * 70)\n",
    "        self.load_all_csv_files()\n",
    "        if not self.csv_files:\n",
    "            return None\n",
    "        self.fix_age_columns()\n",
    "        self.clean_text_columns()\n",
    "        self.combine_all_files()\n",
    "        self.split_month_field()\n",
    "        self.show_sample_data()\n",
    "        self.check_duplicate_data()\n",
    "        self.save_to_snowflake_table()\n",
    "        self.show_snowflake_table_sample()  # Added this line\n",
    "        print(\"üéâ Processing Complete!\")\n",
    "        return self.final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create analyzer\n",
    "analyzer = SimpleCSVAnalyzerSnowflake()\n",
    "\n",
    "# Run full pipeline and get final combined DataFrame\n",
    "df_all = analyzer.run_preprocess()\n",
    "\n",
    "# Preview\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**OUTPUT**\n",
    "\n",
    "üöÄ Starting preprocess, load and clean HDB data  \n",
    "======================================================================  \n",
    "Step 1: Loading ResaleFlat CSV files from Snowflake stage...  \n",
    "==================================================  \n",
    "Found 3 Resale CSV files:  \n",
    "   1. hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   2. hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   3. hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "Columns in this CSV: ['MONTH', 'TOWN', 'FLAT_TYPE', 'BLOCK', 'STREET_NAME', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'LEASE_COMMENCE_DATE', 'REMAINING_LEASE', 'RESALE_PRICE']  \n",
    "‚úÖ Loaded: hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv | Rows: 37,153 | Cols: 11  \n",
    "Columns in this CSV: ['MONTH', 'TOWN', 'FLAT_TYPE', 'BLOCK', 'STREET_NAME', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'LEASE_COMMENCE_DATE', 'RESALE_PRICE']  \n",
    "‚úÖ Loaded: hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv | Rows: 52,203 | Cols: 10  \n",
    "Columns in this CSV: ['MONTH', 'TOWN', 'FLAT_TYPE', 'BLOCK', 'STREET_NAME', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'LEASE_COMMENCE_DATE', 'REMAINING_LEASE', 'RESALE_PRICE']  \n",
    "‚úÖ Loaded: hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv | Rows: 214,893 | Cols: 11  \n",
    "Step 2: Fixing AGE columns...  \n",
    "==================================================  \n",
    "Using current year: 2025  \n",
    "üîß Processing: hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   ‚úÖ Converted 'REMAINING_LEASE' to AGE = 99 - remaining_lease  \n",
    "   Sample values: [29, 34, 35]  \n",
    "üîß Processing: hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   ‚úÖ Created AGE column from 'LEASE_COMMENCE_DATE'  \n",
    "   Formula: AGE = 2025 - LEASE_COMMENCE_DATE  \n",
    "üîß Processing: hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "   ‚úÖ Converted 'REMAINING_LEASE' to AGE = 99 - remaining_lease  \n",
    "   Sample values: [38, 39, 37]  \n",
    "Step 3: Converting text columns to uppercase...  \n",
    "==================================================  \n",
    "   ‚úÖ Converted TOWN to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   ‚úÖ Converted FLAT_TYPE to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   ‚úÖ Converted STREET_NAME to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   ‚úÖ Converted FLAT_MODEL to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv  \n",
    "   ‚úÖ Converted TOWN to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   ‚úÖ Converted FLAT_TYPE to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   ‚úÖ Converted STREET_NAME to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   ‚úÖ Converted FLAT_MODEL to uppercase in hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv  \n",
    "   ‚úÖ Converted TOWN to uppercase in hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "   ‚úÖ Converted FLAT_TYPE to uppercase in hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "   ‚úÖ Converted STREET_NAME to uppercase in hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "   ‚úÖ Converted FLAT_MODEL to uppercase in hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv  \n",
    "Step 5: Combining all files...  \n",
    "==================================================  \n",
    "Step 4: Preparing dataset before combining...  \n",
    "==================================================  \n",
    "   ‚úÖ Columns filtered for hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv: 8 columns kept  \n",
    "   ‚úÖ Columns filtered for hdb_stage/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv: 8 columns kept  \n",
    "   ‚úÖ Columns filtered for hdb_stage/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv: 8 columns kept  \n",
    "‚úÖ All CSV files have consistent columns.  \n",
    "‚úÖ Combined successfully! Rows: 304,249, Cols: 9  \n",
    "Step 6: Splitting MONTH field...  \n",
    "==================================================  \n",
    "‚úÖ Split MONTH into YEAR and MONTH_NUM  \n",
    "Step 7: Sample of filtered data...  \n",
    "==================================================  \n",
    "       MONTH        TOWN  ...  YEAR MONTH_NUM  \n",
    "0 2015-01-01  ANG MO KIO  ...  2015         1  \n",
    "1 2015-01-01  ANG MO KIO  ...  2015         1  \n",
    "2 2015-01-01  ANG MO KIO  ...  2015         1  \n",
    "[3 rows x 11 columns]  \n",
    "Columns: ['MONTH', 'TOWN', 'FLAT_TYPE', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'RESALE_PRICE', 'AGE', 'SOURCE_FILE', 'YEAR', 'MONTH_NUM']  \n",
    "Step 8: Checking for duplicate rows...  \n",
    "==================================================  \n",
    "üîç Found 2,322 duplicate rows. Dropping duplicates...  \n",
    "‚úÖ Remaining rows: 301,927  \n",
    "Step 9: Saving final dataset to Snowflake table...  \n",
    "==================================================  \n",
    "Final columns to save: ['TOWN', 'FLAT_TYPE', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'RESALE_PRICE', 'AGE', 'YEAR', 'MONTH_NUM']  \n",
    "Rows to save: 301,927  \n",
    "‚úÖ Saved final dataset to Snowflake table: HDB_SILVER  \n",
    "Step 10: Showing top 3 rows from Snowflake table HDB_SILVER...  \n",
    "======================================================================  \n",
    "‚úÖ Top 3 rows from HDB_SILVER:  \n",
    "----------------------------------------------------------------------------------------------------  \n",
    "Row 1: {'TOWN': 'WOODLANDS', 'FLAT_TYPE': '4 ROOM', 'STOREY_RANGE': '13 TO 15', 'FLOOR_AREA_SQM': 103, 'FLAT_MODEL': 'PREMIUM APARTMENT', 'RESALE_PRICE': 626000, 'AGE': 26, 'YEAR': 2025, 'MONTH_NUM': 4}  \n",
    "--------------------------------------------------  \n",
    "Row 2: {'TOWN': 'WOODLANDS', 'FLAT_TYPE': '4 ROOM', 'STOREY_RANGE': '01 TO 03', 'FLOOR_AREA_SQM': 104, 'FLAT_MODEL': 'PREMIUM APARTMENT', 'RESALE_PRICE': 600000, 'AGE': 26, 'YEAR': 2025, 'MONTH_NUM': 4}  \n",
    "--------------------------------------------------  \n",
    "Row 3: {'TOWN': 'WOODLANDS', 'FLAT_TYPE': '4 ROOM', 'STOREY_RANGE': '13 TO 15', 'FLOOR_AREA_SQM': 102, 'FLAT_MODEL': 'PREMIUM APARTMENT', 'RESALE_PRICE': 608000, 'AGE': 26, 'YEAR': 2025, 'MONTH_NUM': 4}  \n",
    "--------------------------------------------------  \n",
    "Table columns (9): ['TOWN', 'FLAT_TYPE', 'STOREY_RANGE', 'FLOOR_AREA_SQM', 'FLAT_MODEL', 'RESALE_PRICE', 'AGE', 'YEAR', 'MONTH_NUM']  \n",
    "üéâ Processing Complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
