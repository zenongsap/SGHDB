{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature : Price Estimate using XGBoost, RandomForest, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\sit\\appdata\\roaming\\python\\python312\\site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.5\n",
      "C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\__init__.py\n",
      "xgboost.sklearn\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "print(xgboost.__version__)\n",
    "print(xgboost.__file__)\n",
    "print(XGBRegressor.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.5\n",
      "xgboost.sklearn\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(xgboost.__version__)  # should be 3.1+ (or latest)\n",
    "print(XGBRegressor.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sit\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Applying one-hot encoding on: ['TOWN', 'FLAT_TYPE']\n",
      "‚úÖ One-hot encoding complete.\n",
      "üìå Encoded columns preview: ['FLOOR_AREA_SQM', 'RESALE_PRICE', 'AGE', 'STOREY_NUMERIC', 'DATE_IDX', 'TOWN_ANG MO KIO', 'TOWN_BEDOK', 'TOWN_BISHAN', 'TOWN_BUKIT BATOK', 'TOWN_BUKIT MERAH', 'TOWN_BUKIT PANJANG', 'TOWN_BUKIT TIMAH', 'TOWN_CENTRAL AREA', 'TOWN_CHOA CHU KANG', 'TOWN_CLEMENTI', 'TOWN_GEYLANG', 'TOWN_HOUGANG', 'TOWN_JURONG EAST', 'TOWN_JURONG WEST', 'TOWN_KALLANG/WHAMPOA']\n",
      "   FLOOR_AREA_SQM  RESALE_PRICE  AGE  STOREY_NUMERIC  DATE_IDX  \\\n",
      "0              59     12.842652   50               8     24150   \n",
      "1              65     12.906694   50               8     24150   \n",
      "2              65     12.945629   50               8     24150   \n",
      "3              65     12.985400   50               8     24150   \n",
      "4              68     12.994532   49               8     24150   \n",
      "\n",
      "   TOWN_ANG MO KIO  TOWN_BEDOK  TOWN_BISHAN  TOWN_BUKIT BATOK  \\\n",
      "0                0           0            0                 0   \n",
      "1                0           0            0                 0   \n",
      "2                0           0            0                 0   \n",
      "3                0           0            0                 0   \n",
      "4                0           0            0                 0   \n",
      "\n",
      "   TOWN_BUKIT MERAH  ...  TOWN_SERANGOON  TOWN_TAMPINES  TOWN_TOA PAYOH  \\\n",
      "0                 0  ...               0              0               0   \n",
      "1                 0  ...               0              0               0   \n",
      "2                 0  ...               0              0               0   \n",
      "3                 0  ...               0              0               0   \n",
      "4                 0  ...               0              0               0   \n",
      "\n",
      "   TOWN_WOODLANDS  TOWN_YISHUN  FLAT_TYPE_3 ROOM  FLAT_TYPE_4 ROOM  \\\n",
      "0               0            0                 1                 0   \n",
      "1               0            0                 1                 0   \n",
      "2               0            0                 1                 0   \n",
      "3               0            0                 1                 0   \n",
      "4               0            0                 1                 0   \n",
      "\n",
      "   FLAT_TYPE_5 ROOM  FLAT_TYPE_EXECUTIVE  FLAT_TYPE_MULTI-GENERATION  \n",
      "0                 0                    0                           0  \n",
      "1                 0                    0                           0  \n",
      "2                 0                    0                           0  \n",
      "3                 0                    0                           0  \n",
      "4                 0                    0                           0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "‚úÖ train set numeric dtypes: int32    31\n",
      "int64     4\n",
      "Name: count, dtype: int64\n",
      "‚úÖ valid set numeric dtypes: int32    31\n",
      "int64     4\n",
      "Name: count, dtype: int64\n",
      "‚úÖ test set numeric dtypes: int32    31\n",
      "int64     4\n",
      "Name: count, dtype: int64\n",
      "11:22:09 \n",
      "‚úÖ Data ready for training (train/valid/test) - 11:22:09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# ======================================================\n",
    "# 1. Load data\n",
    "# ========================================================\n",
    "df = pd.read_csv('raw_data_main.csv')\n",
    "\n",
    "# Exclude outliers (remove rows where IS_OUTLIERS = 1)\n",
    "df = df[df['IS_OUTLIERS'] != 1]\n",
    "\n",
    "# Create DATE_IDX (optional)\n",
    "df['DATE_IDX'] = df['YEAR'] * 12 + df['MONTH_NUM']\n",
    "\n",
    "# Log-transform target\n",
    "df['RESALE_PRICE'] = np.log1p(df['RESALE_PRICE'])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. Drop unwanted columns BEFORE preparing features\n",
    "# --------------------------------------------------------\n",
    "drop_cols = ['IS_OUTLIERS', 'STOREY_RANGE', 'PRICE_PER_SQM', 'YEAR', 'MONTH_NUM','PRICE_TIER','SEASON','AGE_GROUP']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Define categorical variables to encode\n",
    "categorical_cols = ['TOWN', 'FLAT_TYPE']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "print(f\"üìå Applying one-hot encoding on: {categorical_cols}\")\n",
    "df = pd.get_dummies(df, columns=categorical_cols, dtype=int)\n",
    "print(\"‚úÖ One-hot encoding complete.\")\n",
    "print(\"üìå Encoded columns preview:\", df.columns.tolist()[:20])\n",
    "print(df.head())\n",
    "\n",
    "# Optional: sample smaller subset for quick experiments\n",
    "df = df.sample(20000, random_state=42)\n",
    "\n",
    "# Create bin for stratified sampling\n",
    "df['price_bin'] = pd.qcut(df['RESALE_PRICE'], q=4, labels=False)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. Train / Validation / Test split\n",
    "# --------------------------------------------------------\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['price_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train, df_valid = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.25,\n",
    "    stratify=df_trainval['price_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Drop helper column used for stratification\n",
    "df_train = df_train.drop(columns=['price_bin'])\n",
    "df_valid = df_valid.drop(columns=['price_bin'])\n",
    "df_test  = df_test.drop(columns=['price_bin'])\n",
    "\n",
    "# ========================================================\n",
    "# 4. Prepare features and target (no further dropping needed)\n",
    "# ========================================================\n",
    "X_train = df_train.drop(columns=['RESALE_PRICE'])\n",
    "y_train = df_train['RESALE_PRICE']\n",
    "\n",
    "X_valid = df_valid.drop(columns=['RESALE_PRICE'])\n",
    "y_valid = df_valid['RESALE_PRICE']\n",
    "\n",
    "X_test  = df_test.drop(columns=['RESALE_PRICE'])\n",
    "y_test  = df_test['RESALE_PRICE']\n",
    "\n",
    "# Ensure all numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_valid = X_valid.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_test  = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Optional: sanity check\n",
    "for name, dfX in [(\"train\", X_train), (\"valid\", X_valid), (\"test\", X_test)]:\n",
    "    print(f\"‚úÖ {name} set numeric dtypes:\", dfX.dtypes.value_counts())\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(f\"{timestamp} \\n‚úÖ Data ready for training (train/valid/test) - {timestamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "11:08:45\n",
      "üìä Random Forest Top Results:\n",
      "   mean_test_score  std_test_score  \\\n",
      "4         0.857309        0.001226   \n",
      "5         0.857307        0.001092   \n",
      "7         0.857258        0.001224   \n",
      "8         0.857254        0.001076   \n",
      "3         0.857198        0.001287   \n",
      "\n",
      "                                                            params  \n",
      "4   {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 800}  \n",
      "5  {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 1200}  \n",
      "7   {'max_depth': 40, 'min_samples_split': 5, 'n_estimators': 800}  \n",
      "8  {'max_depth': 40, 'min_samples_split': 5, 'n_estimators': 1200}  \n",
      "3   {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 500}  \n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "12 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.4 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.88666607 0.88965714 0.87493543 0.87979344 0.88384785 0.8871855\n",
      " 0.87274591 0.87611679        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:08:45\n",
      "üìä XGBoost Top Results:\n",
      "   mean_test_score  std_test_score  \\\n",
      "1         0.889657        0.002858   \n",
      "5         0.887185        0.002575   \n",
      "0         0.886666        0.002386   \n",
      "4         0.883848        0.002466   \n",
      "3         0.879793        0.003004   \n",
      "\n",
      "                                                                                                     params  \n",
      "1   {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}  \n",
      "5   {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}  \n",
      "0   {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 0.8}  \n",
      "4   {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 0.8}  \n",
      "3  {'colsample_bytree': 0.8, 'learning_rate': 0.21, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}  \n",
      "\n",
      "üèÜ Combined Top 5 Results (RF + XGB):\n",
      "   mean_test_score  std_test_score  \\\n",
      "0         0.889657        0.002858   \n",
      "1         0.887185        0.002575   \n",
      "2         0.886666        0.002386   \n",
      "3         0.883848        0.002466   \n",
      "4         0.879793        0.003004   \n",
      "5         0.857309        0.001226   \n",
      "6         0.857307        0.001092   \n",
      "7         0.857258        0.001224   \n",
      "8         0.857254        0.001076   \n",
      "9         0.857198        0.001287   \n",
      "\n",
      "                                                                                                     params  \\\n",
      "0   {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}   \n",
      "1   {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}   \n",
      "2   {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 0.8}   \n",
      "3   {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 0.8}   \n",
      "4  {'colsample_bytree': 0.8, 'learning_rate': 0.21, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}   \n",
      "5                                            {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 800}   \n",
      "6                                           {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 1200}   \n",
      "7                                            {'max_depth': 40, 'min_samples_split': 5, 'n_estimators': 800}   \n",
      "8                                           {'max_depth': 40, 'min_samples_split': 5, 'n_estimators': 1200}   \n",
      "9                                            {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 500}   \n",
      "\n",
      "          model  \n",
      "0       XGBoost  \n",
      "1       XGBoost  \n",
      "2       XGBoost  \n",
      "3       XGBoost  \n",
      "4       XGBoost  \n",
      "5  RandomForest  \n",
      "6  RandomForest  \n",
      "7  RandomForest  \n",
      "8  RandomForest  \n",
      "9  RandomForest  \n",
      "\n",
      "üìå Grid Search completed at 2025-09-15 11:13:52\n",
      "2025-09-15 11:13:52\n",
      "‚úÖ Best Random Forest params:\n",
      "{'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 800}\n",
      "R¬≤: 0.8573\n",
      "2025-09-15 11:13:52\n",
      "‚úÖ Best XGBoost params:\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "R¬≤: 0.8897\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    105\u001b[39m xgb_best = xgb.XGBRegressor(\n\u001b[32m    106\u001b[39m     **xgb_best_params,\n\u001b[32m    107\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m    111\u001b[39m )\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Early stopping on validation set\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mxgb_best\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    119\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# ========================================================\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# 7Ô∏è‚É£ Evaluate function\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# ========================================================\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(model, X, y, label=\u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# =================================================================================================\n",
    "# Training on Random Forest GridSearch and XGBoost GridSearch to find best result\n",
    "# =================================================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# ========================================================\n",
    "# 2Ô∏è‚É£ Random Forest GridSearch\n",
    "# ========================================================\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [500, 800, 1200],\n",
    "    'max_depth': [25, 30, 40],\n",
    " #   'min_samples_split': [2, 5, 10]\n",
    "    'min_samples_split': [5]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_results = pd.DataFrame(rf_grid.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "print(f\"{timestamp}\\nüìä Random Forest Top Results:\")\n",
    "print(rf_results[['mean_test_score','std_test_score','params']].head())\n",
    "\n",
    "# ========================================================\n",
    "# 3Ô∏è‚É£ XGBoost GridSearch (without early stopping)\n",
    "# ========================================================\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_param_grid = {\n",
    "#    'n_estimators': [800, 1200, 1500],\n",
    "    'n_estimators': [1200],\n",
    "    'max_depth': [6],\n",
    "#    'max_depth': [6, 10, 12],\n",
    "#    'learning_rate': [0.01, 0.05, 0.1],\n",
    "     'learning_rate': [0.1, 0.21],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0,1.4]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "xgb_results = pd.DataFrame(xgb_grid.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "print(f\"{timestamp}\\nüìä XGBoost Top Results:\")\n",
    "print(xgb_results[['mean_test_score','std_test_score','params']].head())\n",
    "\n",
    "# ========================================================\n",
    "# 4Ô∏è‚É£ Combine Top Results\n",
    "# ========================================================\n",
    "rf_top = rf_results[['mean_test_score','std_test_score','params']].head().copy()\n",
    "rf_top['model'] = 'RandomForest'\n",
    "\n",
    "xgb_top = xgb_results[['mean_test_score','std_test_score','params']].head().copy()\n",
    "xgb_top['model'] = 'XGBoost'\n",
    "\n",
    "combined_top = pd.concat([rf_top, xgb_top]).sort_values(by='mean_test_score', ascending=False).reset_index(drop=True)\n",
    "print(\"\\nüèÜ Combined Top 5 Results (RF + XGB):\")\n",
    "print(combined_top)\n",
    "\n",
    "# ========================================================\n",
    "# 5Ô∏è‚É£ Print best params\n",
    "# ========================================================\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nüìå Grid Search completed at {timestamp}\")\n",
    "\n",
    "print(f\"{timestamp}\\n‚úÖ Best Random Forest params:\")\n",
    "print(rf_grid.best_params_)\n",
    "print(f\"R¬≤: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"{timestamp}\\n‚úÖ Best XGBoost params:\")\n",
    "print(xgb_grid.best_params_)\n",
    "print(f\"R¬≤: {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "# ========================================================\n",
    "# 6Ô∏è‚É£ Retrain XGBoost best model with Early Stopping\n",
    "# ========================================================\n",
    "xgb_best_params = xgb_grid.best_params_\n",
    "xgb_best = xgb.XGBRegressor(\n",
    "    **xgb_best_params,\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Early stopping on validation set\n",
    "xgb_best.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ========================================================\n",
    "# 7Ô∏è‚É£ Evaluate function\n",
    "# ========================================================\n",
    "def evaluate(model, X, y, label=\"Model\"):\n",
    "    preds = np.expm1(model.predict(X))\n",
    "    y_true = np.expm1(y)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    print(f\"\\nüìä {label} Test Metrics:\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "evaluate(rf_grid.best_estimator_, X_test, y_test, label=\"Random Forest\")\n",
    "evaluate(xgb_best, X_test, y_test, label=\"XGBoost with Early Stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Starting GridSearch for LightGBM at 2025-09-15 11:16:03\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 421\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 13.077856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "‚úÖ LightGBM GridSearch completed at 2025-09-15 11:17:09\n",
      "\n",
      "‚è≥ Starting GridSearch for CatBoost at 2025-09-15 11:17:09\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "‚úÖ CatBoost GridSearch completed at 2025-09-15 11:18:32\n",
      "\n",
      "üìå Final Results Summary at 2025-09-15 11:18:32\n",
      "\n",
      "‚úÖ Best LightGBM params:\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 800, 'subsample': 0.3}\n",
      "R¬≤: 0.8853\n",
      "\n",
      "‚úÖ Best CatBoost params:\n",
      "{'depth': 6, 'iterations': 1200, 'l2_leaf_reg': 3, 'learning_rate': 0.1}\n",
      "R¬≤: 0.8944\n",
      "\n",
      "üìä LightGBM Test Metrics at 2025-09-15 11:18:32:\n",
      "RMSE: 53,620.99 | MAE: 37,182.16 | R¬≤: 0.9012\n",
      "\n",
      "üìä CatBoost Test Metrics at 2025-09-15 11:18:32:\n",
      "RMSE: 52,282.69 | MAE: 36,174.33 | R¬≤: 0.9061\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ========================================================\n",
    "# üîç Training GridSearchCV for LightGBM and CatBoost\n",
    "# ========================================================\n",
    "print(f\"\\n‚è≥ Starting GridSearch for LightGBM at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "lgb_model = LGBMRegressor(objective='regression', random_state=42, n_jobs=-1)\n",
    "\n",
    "lgb_param_grid = {\n",
    "#    'n_estimators': [500, 800],\n",
    "    'n_estimators': [800, 1200],\n",
    "    'max_depth': [6, 12],\n",
    "#    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'learning_rate': [0.1],\n",
    "#    'subsample': [0.8, 1.0],\n",
    "#    'colsample_bytree': [0.8, 1.0]\n",
    "    'subsample': [0.3, 0.8],\n",
    "    'colsample_bytree': [0.3, 0.8]\n",
    "}\n",
    "\n",
    "lgb_grid = GridSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_grid=lgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ LightGBM GridSearch completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "print(f\"\\n‚è≥ Starting GridSearch for CatBoost at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "cat_param_grid = {\n",
    "#    'iterations': [500, 800],\n",
    "    'iterations': [800,1200],\n",
    "    'depth': [6, 10],\n",
    "#    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'learning_rate': [0.1],\n",
    "#    'l2_leaf_reg': [3, 5]\n",
    "     'l2_leaf_reg': [3, 8]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=cat_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cat_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ CatBoost GridSearch completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Results Summary\n",
    "# --------------------------------------------------------\n",
    "print(f\"\\nüìå Final Results Summary at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best LightGBM params:\")\n",
    "print(lgb_grid.best_params_)\n",
    "print(f\"R¬≤: {lgb_grid.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best CatBoost params:\")\n",
    "print(cat_grid.best_params_)\n",
    "print(f\"R¬≤: {cat_grid.best_score_:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Evaluation Function\n",
    "# --------------------------------------------------------\n",
    "def evaluate(model, X, y, label=\"Model\"):\n",
    "    preds = np.expm1(model.predict(X))\n",
    "    y_true = np.expm1(y)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    print(f\"\\nüìä {label} Test Metrics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}:\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Final Evaluation on Test Set\n",
    "# --------------------------------------------------------\n",
    "evaluate(lgb_grid.best_estimator_, X_test, y_test, label=\"LightGBM\")\n",
    "evaluate(cat_grid.best_estimator_, X_test, y_test, label=\"CatBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Starting Weighted Ensemble Evaluation at 2025-09-15 11:18:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sit\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\callback.py:386: UserWarning: [11:18:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Weighted Ensemble Test Metrics at 2025-09-15 11:18:51:\n",
      "RMSE: 51,771.76 | MAE: 35,828.82 | R¬≤: 0.9079\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üîç  Weighted Ensemble Evaluation for XGB, LightGBM, CatBoost\n",
    "# ========================================================\n",
    "\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(f\"\\n‚è≥ Starting Weighted Ensemble Evaluation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Convert to DMatrix\n",
    "# --------------------------------------------------------\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest  = xgb.DMatrix(X_test)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Set Parameters\n",
    "# --------------------------------------------------------\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 1200,  # used in num_boost_round\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Train with Early Stopping\n",
    "# --------------------------------------------------------\n",
    "xgb_model = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1200,\n",
    "    evals=[(dvalid, 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Individual Predictions (reverse log-transform)\n",
    "# --------------------------------------------------------\n",
    "pred_cat = np.expm1(cat_grid.best_estimator_.predict(X_test))\n",
    "pred_xgb = np.expm1(xgb_model.predict(dtest))\n",
    "pred_lgb = np.expm1(lgb_grid.best_estimator_.predict(X_test))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Weighted Ensemble Prediction\n",
    "# --------------------------------------------------------\n",
    "ensemble_pred = (0.7 * pred_cat + 0.2 * pred_xgb + 0.1 * pred_lgb)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Evaluation\n",
    "# --------------------------------------------------------\n",
    "rmse = np.sqrt(mean_squared_error(y_true, ensemble_pred))\n",
    "mae = mean_absolute_error(y_true, ensemble_pred)\n",
    "r2 = r2_score(y_true, ensemble_pred)\n",
    "\n",
    "print(f\"\\nüìä Weighted Ensemble Test Metrics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}:\")\n",
    "print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Starting Validation on df_valid at 2025-09-12 14:36:32\n",
      "\n",
      "üìä Weighted Ensemble Validation Metrics at 2025-09-12 14:36:33:\n",
      "RMSE: 47,827.53 | MAE: 32,419.19 | R¬≤: 0.9180\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Validating model with separate data\n",
    "# ========================================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"\\n‚è≥ Starting Validation on df_valid at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Prepare validation features and target\n",
    "# --------------------------------------------------------\n",
    "X_valid_eval = df_valid.drop(columns=['RESALE_PRICE', 'price_bin'], errors='ignore')\n",
    "y_valid_eval = df_valid['RESALE_PRICE']\n",
    "\n",
    "# Ensure numeric\n",
    "X_valid_eval = X_valid_eval.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Convert to DMatrix for XGBoost\n",
    "dvalid_eval = xgb.DMatrix(X_valid_eval)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Individual Predictions (reverse log-transform)\n",
    "# --------------------------------------------------------\n",
    "pred_cat_val = np.expm1(cat_grid.best_estimator_.predict(X_valid_eval))\n",
    "pred_xgb_val = np.expm1(xgb_model.predict(dvalid_eval))\n",
    "pred_lgb_val = np.expm1(lgb_grid.best_estimator_.predict(X_valid_eval))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Weighted Ensemble Prediction\n",
    "# --------------------------------------------------------\n",
    "ensemble_pred_val = (0.4 * pred_cat_val + 0.3 * pred_xgb_val + 0.3 * pred_lgb_val)\n",
    "y_true_val = np.expm1(y_valid_eval)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Evaluation\n",
    "# --------------------------------------------------------\n",
    "rmse_val = np.sqrt(mean_squared_error(y_true_val, ensemble_pred_val))\n",
    "mae_val = mean_absolute_error(y_true_val, ensemble_pred_val)\n",
    "r2_val = r2_score(y_true_val, ensemble_pred_val)\n",
    "\n",
    "print(f\"\\nüìä Weighted Ensemble Validation Metrics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}:\")\n",
    "print(f\"RMSE: {rmse_val:,.2f} | MAE: {mae_val:,.2f} | R¬≤: {r2_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "‚úÖ Best CatBoost params: {'bagging_temperature': 0, 'border_count': 128, 'depth': 6, 'grow_policy': 'SymmetricTree', 'iterations': 1600, 'l2_leaf_reg': 3, 'learning_rate': 0.05}\n",
      "R¬≤: 0.8793\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üîç Training CatBoost with GridSearchCV\n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "df = pd.read_csv('raw_data_main.csv')\n",
    "\n",
    "# Optional: sample smaller subset for quick experiments\n",
    "df = df.sample(10000, random_state=42)\n",
    "\n",
    "# Create DATE_IDX (optional)\n",
    "df['DATE_IDX'] = df['YEAR'] * 12 + df['MONTH_NUM']\n",
    "\n",
    "# Log-transform target\n",
    "df['RESALE_PRICE'] = np.log1p(df['RESALE_PRICE'])\n",
    "\n",
    "# Drop unwanted columns BEFORE feature prep\n",
    "drop_cols = ['IS_OUTLIERS', 'STOREY_RANGE', 'PRICE_PER_SQM', 'YEAR', 'MONTH_NUM', 'PRICE_TIER', 'SEASON', 'AGE_GROUP']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# 2. Split data\n",
    "X = df.drop(columns=['RESALE_PRICE'])\n",
    "y = df['RESALE_PRICE']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 3. Convert categorical features (assumes these columns are categorical but NOT one-hot encoded)\n",
    "cat_features = ['TOWN', 'FLAT_TYPE']  # Add other categorical feature names here as needed\n",
    "\n",
    "for col in cat_features:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_valid[col] = X_valid[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# 4. Define CatBoost model & parameter grid\n",
    "cat_model = CatBoostRegressor(\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "'''\n",
    "cat_param_grid = {\n",
    "    'iterations': [800, 1200, 1600],\n",
    "    'depth': [6, 10],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 8, 10],\n",
    "    'bagging_temperature': [0, 1],\n",
    "    'border_count': [64, 128, 254],\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise']\n",
    "}\n",
    "'''\n",
    "cat_param_grid = {\n",
    "    'iterations': [1200,1600],\n",
    "    'depth': [6,10],\n",
    "    'learning_rate': [0.05,0.1],\n",
    "    'l2_leaf_reg': [3],\n",
    "    'bagging_temperature': [0],\n",
    "    'border_count': [128,254],\n",
    "    'grow_policy': ['SymmetricTree']\n",
    "}\n",
    "# 5. Grid Search CV\n",
    "cat_grid = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=cat_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "cat_grid.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_valid, y_valid), early_stopping_rounds=50)\n",
    "\n",
    "print(f\"\\n‚úÖ Best CatBoost params: {cat_grid.best_params_}\")\n",
    "print(f\"R¬≤: {cat_grid.best_score_:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected categorical features: []\n",
      "üîç Running Bayesian Optimization for CatBoost...\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1437, 'depth': 7, 'learning_rate': 0.10678146001636617, 'l2_leaf_reg': 5, 'bagging_temperature': 0.44583275285359125, 'border_count': 83, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9354538710284577\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1067, 'depth': 7, 'learning_rate': 0.09905330837693117, 'l2_leaf_reg': 3, 'bagging_temperature': 0.7219987722668249, 'border_count': 242, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9359597123371913\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1594, 'depth': 8, 'learning_rate': 0.09669918962929686, 'l2_leaf_reg': 3, 'bagging_temperature': 0.02306242504141576, 'border_count': 164, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9367703586060451\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 837, 'depth': 10, 'learning_rate': 0.07396628042581825, 'l2_leaf_reg': 3, 'bagging_temperature': 0.6183860093330874, 'border_count': 137, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9341427682311846\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1173, 'depth': 9, 'learning_rate': 0.10081845231526679, 'l2_leaf_reg': 4, 'bagging_temperature': 0.013264961159866532, 'border_count': 243, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9337068684906525\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 874, 'depth': 10, 'learning_rate': 0.061751851321168845, 'l2_leaf_reg': 6, 'bagging_temperature': 0.9899576959639305, 'border_count': 194, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9339872216322392\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 801, 'depth': 7, 'learning_rate': 0.08113744267711727, 'l2_leaf_reg': 6, 'bagging_temperature': 0.3401742119135485, 'border_count': 66, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9340334358400177\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1598, 'depth': 7, 'learning_rate': 0.11293717896989258, 'l2_leaf_reg': 6, 'bagging_temperature': 0.9235489883404009, 'border_count': 187, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9346768202773543\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1539, 'depth': 6, 'learning_rate': 0.10966746767505078, 'l2_leaf_reg': 6, 'bagging_temperature': 0.004383732594958945, 'border_count': 241, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9364170355338195\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1575, 'depth': 10, 'learning_rate': 0.08865473307254, 'l2_leaf_reg': 6, 'bagging_temperature': 0.0, 'border_count': 254, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9361300259033392\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1357, 'depth': 8, 'learning_rate': 0.0617546588653247, 'l2_leaf_reg': 6, 'bagging_temperature': 0.9435218991372326, 'border_count': 235, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9353564126964866\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1280, 'depth': 9, 'learning_rate': 0.0687445067910117, 'l2_leaf_reg': 4, 'bagging_temperature': 0.031067115233438643, 'border_count': 69, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9361588011761692\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 944, 'depth': 7, 'learning_rate': 0.11721051664670756, 'l2_leaf_reg': 3, 'bagging_temperature': 0.9951223099765325, 'border_count': 197, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9347837086777808\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1498, 'depth': 10, 'learning_rate': 0.11690504365928395, 'l2_leaf_reg': 6, 'bagging_temperature': 0.9562743816437711, 'border_count': 188, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9351252267435327\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1117, 'depth': 9, 'learning_rate': 0.1193768449860557, 'l2_leaf_reg': 3, 'bagging_temperature': 0.9668874622187451, 'border_count': 69, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9343794532677957\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 809, 'depth': 9, 'learning_rate': 0.11330685080341264, 'l2_leaf_reg': 6, 'bagging_temperature': 0.23151933294077082, 'border_count': 221, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9361660586399049\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 946, 'depth': 6, 'learning_rate': 0.060363448797351955, 'l2_leaf_reg': 4, 'bagging_temperature': 0.9781440782441698, 'border_count': 70, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9343052485957339\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 877, 'depth': 8, 'learning_rate': 0.11746549626757538, 'l2_leaf_reg': 3, 'bagging_temperature': 0.022589713644876722, 'border_count': 74, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9341086652098597\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1594, 'depth': 9, 'learning_rate': 0.1192133944045476, 'l2_leaf_reg': 4, 'bagging_temperature': 0.9741263293715725, 'border_count': 72, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9339583109607301\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1280, 'depth': 8, 'learning_rate': 0.06014371594389748, 'l2_leaf_reg': 3, 'bagging_temperature': 0.027035449521464534, 'border_count': 120, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9357008251189128\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1113, 'depth': 7, 'learning_rate': 0.08812843557861735, 'l2_leaf_reg': 6, 'bagging_temperature': 0.0, 'border_count': 64, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9355326342364476\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1055, 'depth': 10, 'learning_rate': 0.10215493448799731, 'l2_leaf_reg': 4, 'bagging_temperature': 0.0, 'border_count': 194, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9341633532501414\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1600, 'depth': 6, 'learning_rate': 0.08195034467551474, 'l2_leaf_reg': 4, 'bagging_temperature': 0.14115530618458705, 'border_count': 254, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9357702342425784\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1598, 'depth': 7, 'learning_rate': 0.10960244663287794, 'l2_leaf_reg': 4, 'bagging_temperature': 0.6689137734461962, 'border_count': 79, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9360312240865808\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1590, 'depth': 9, 'learning_rate': 0.11830259508533993, 'l2_leaf_reg': 5, 'bagging_temperature': 0.28203685549537744, 'border_count': 65, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9360296541805808\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1581, 'depth': 6, 'learning_rate': 0.06932684362486496, 'l2_leaf_reg': 6, 'bagging_temperature': 0.1209250532526163, 'border_count': 160, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.935697226384242\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1498, 'depth': 10, 'learning_rate': 0.06307249988563575, 'l2_leaf_reg': 4, 'bagging_temperature': 0.5604012485145161, 'border_count': 246, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9356418999907911\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1217, 'depth': 6, 'learning_rate': 0.06102782747576826, 'l2_leaf_reg': 5, 'bagging_temperature': 0.9893221091277027, 'border_count': 108, 'grow_policy': 'SymmetricTree'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9336979333057818\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1303, 'depth': 8, 'learning_rate': 0.06623713025302867, 'l2_leaf_reg': 6, 'bagging_temperature': 0.29708817079158306, 'border_count': 246, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9360130898703795\n",
      "\n",
      "--- New Iteration ---\n",
      "Params: {'iterations': 1449, 'depth': 10, 'learning_rate': 0.08674327767112108, 'l2_leaf_reg': 4, 'bagging_temperature': 0.9667833335101117, 'border_count': 208, 'grow_policy': 'Depthwise'}\n",
      "X_train shape: (12000, 35) X_valid shape: (4000, 35)\n",
      "cat_features: []\n",
      "R¬≤: 0.9342344032459607\n",
      "\n",
      "‚è±Ô∏è Optimization finished in 0:04:11.982146\n",
      "üìÅ Saved best parameters to 'best_catboost_params.json'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üîç Bayesian Optimization + Train CatBoost + Evaluate\n",
    "# ========================================================\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure X_train / X_valid are DataFrames\n",
    "# -----------------------------\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "if not isinstance(X_valid, pd.DataFrame):\n",
    "    X_valid = pd.DataFrame(X_valid, columns=X_columns)\n",
    "\n",
    "# -----------------------------\n",
    "# Detect categorical columns automatically\n",
    "# -----------------------------\n",
    "# Replace with your actual categorical types if needed\n",
    "cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Detected categorical features:\", cat_features)\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameter space\n",
    "# -----------------------------\n",
    "param_space = [\n",
    "    Integer(800, 1600, name='iterations'),\n",
    "    Integer(6, 10, name='depth'),\n",
    "    Real(0.06, 0.12, name='learning_rate'),\n",
    "    Integer(3, 6, name='l2_leaf_reg'),\n",
    "    Real(0, 1, name='bagging_temperature'),\n",
    "    Integer(64, 254, name='border_count'),\n",
    "    Categorical(['SymmetricTree', 'Depthwise'], name='grow_policy')\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Objective function with debug\n",
    "# -----------------------------\n",
    "@use_named_args(param_space)\n",
    "def objective(**params):\n",
    "    print(\"\\n--- New Iteration ---\")\n",
    "    print(\"Params:\", params)\n",
    "    print(\"X_train shape:\", X_train.shape, \"X_valid shape:\", X_valid.shape)\n",
    "    print(\"cat_features:\", cat_features)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        verbose=0,\n",
    "        random_state=42,\n",
    "        loss_function='RMSE',\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_features,\n",
    "        eval_set=(X_valid, y_valid),\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_valid)\n",
    "    r2 = r2_score(y_valid, preds)\n",
    "    print(\"R¬≤:\", r2)\n",
    "    return -r2  # minimize negative R¬≤\n",
    "\n",
    "# -----------------------------\n",
    "# Run Bayesian Optimization\n",
    "# -----------------------------\n",
    "print(\"üîç Running Bayesian Optimization for CatBoost...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "results = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=param_space,\n",
    "    n_calls=30,\n",
    "    n_initial_points=5,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\n‚è±Ô∏è Optimization finished in {end_time - start_time}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Extract best parameters\n",
    "# -----------------------------\n",
    "best_r2 = -results.fun\n",
    "best_params = {dim.name: val for dim, val in zip(param_space, results.x)}\n",
    "best_params['r2'] = best_r2\n",
    "\n",
    "# Convert NumPy types to native Python types\n",
    "best_params_clean = {\n",
    "    k: float(v) if isinstance(v, (np.float32, np.float64))\n",
    "    else int(v) if isinstance(v, (np.int32, np.int64))\n",
    "    else v\n",
    "    for k, v in best_params.items()\n",
    "}\n",
    "\n",
    "# Save best parameters\n",
    "with open(\"best_catboost_params.json\", \"w\") as f:\n",
    "    json.dump(best_params_clean, f, indent=4)\n",
    "print(\"üìÅ Saved best parameters to 'best_catboost_params.json'\")\n",
    "\n",
    "# -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, label=\"Model\"):\n",
    "    preds = np.expm1(model.predict(X))\n",
    "    y_true = np.expm1(y)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    print(f\"\\nüìä {label} Test Metrics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}:\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3087147\ttest: 0.3051864\tbest: 0.3051864 (0)\ttotal: 116ms\tremaining: 3m 5s\n",
      "1000:\tlearn: 0.0721279\ttest: 0.0887405\tbest: 0.0887405 (1000)\ttotal: 1m 47s\tremaining: 1m 4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.08853360709\n",
      "bestIteration = 1318\n",
      "\n",
      "Shrink model to first 1319 iterations.\n",
      "\n",
      "üìä CatBoost Validation Test Metrics at 2025-09-15 12:18:30:\n",
      "RMSE: 47,910.55 | MAE: 32,354.26 | R¬≤: 0.9177\n",
      "\n",
      "üìä Validation Results\n",
      "RMSE: 51,771.76 | MAE: 35,828.82 | R¬≤: 0.9079\n",
      "‚úÖ Trained CatBoost model saved to 'catboost_model_valid.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üß† Load best CatBoost parameters and train model\n",
    "# ========================================================\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load best parameters from JSON\n",
    "with open(\"best_catboost_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Remove R¬≤ score from params dict if it exists\n",
    "best_params.pop('r2', None)\n",
    "\n",
    "# 2. Train the model on X_train\n",
    "model = CatBoostRegressor(\n",
    "    verbose=1000,\n",
    "    random_state=42,\n",
    "    loss_function='RMSE',\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_valid, y_valid),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# 3. Predict on validation set\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "# 4. Evaluate model\n",
    "evaluate(model, X_valid, y_valid, label=\"CatBoost Validation\")\n",
    "\n",
    "print(f\"\\nüìä Validation Results\")\n",
    "print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "# 5. (Optional) Save the trained model\n",
    "joblib.dump(model, \"catboost_model_valid.pkl\")\n",
    "print(\"‚úÖ Trained CatBoost model saved to 'catboost_model_valid.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CatBoost_features_used.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the list of features actually used in the model\n",
    "joblib.dump(features_used, \"CatBoost_features_used.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved weighted ensemble to 'E_Price_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Download trained model on ensemble_model (TRIED, discard because too complex)\n",
    "# ========================================================\n",
    "import pickle\n",
    "\n",
    "ensemble_model = {\n",
    "    \"catboost\": cat_model,\n",
    "    \"xgboost_native\": xgb_native_model,\n",
    "    \"lightgbm\": lgb_model,\n",
    "    \"weights\": {\n",
    "        \"catboost\": 0.7,\n",
    "        \"xgboost_native\": 0.2,\n",
    "        \"lightgbm\": 0.1\n",
    "    },\n",
    "    \"feature_names\": list(X_train.columns)  # Save feature order\n",
    "}\n",
    "\n",
    "with open(\"E_Price_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "\n",
    "print(\"‚úÖ Saved weighted ensemble to 'E_Price_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Applying one-hot encoding on: ['TOWN', 'FLAT_TYPE']\n",
      "‚úÖ One-hot encoding complete.\n",
      "üìå Encoded columns preview: ['FLOOR_AREA_SQM', 'RESALE_PRICE', 'AGE', 'YEAR', 'STOREY_NUMERIC', 'DATE_IDX', 'TOWN_ANG MO KIO', 'TOWN_BEDOK', 'TOWN_BISHAN', 'TOWN_BUKIT BATOK', 'TOWN_BUKIT MERAH', 'TOWN_BUKIT PANJANG', 'TOWN_BUKIT TIMAH', 'TOWN_CENTRAL AREA', 'TOWN_CHOA CHU KANG', 'TOWN_CLEMENTI', 'TOWN_GEYLANG', 'TOWN_HOUGANG', 'TOWN_JURONG EAST', 'TOWN_JURONG WEST']\n",
      "   FLOOR_AREA_SQM  RESALE_PRICE  AGE  YEAR  STOREY_NUMERIC  DATE_IDX  \\\n",
      "0              91     12.971543   68  2023               2     24280   \n",
      "1              74     12.971543   63  2023               8     24280   \n",
      "2              84     13.012551   62  2023               5     24280   \n",
      "3              84     13.017005   64  2023               5     24280   \n",
      "4              89     13.023650   62  2023               2     24280   \n",
      "\n",
      "   TOWN_ANG MO KIO  TOWN_BEDOK  TOWN_BISHAN  TOWN_BUKIT BATOK  ...  \\\n",
      "0                0           0            0                 0  ...   \n",
      "1                0           0            0                 0  ...   \n",
      "2                0           0            0                 0  ...   \n",
      "3                0           0            0                 0  ...   \n",
      "4                0           0            0                 0  ...   \n",
      "\n",
      "   TOWN_SERANGOON  TOWN_TAMPINES  TOWN_TOA PAYOH  TOWN_WOODLANDS  TOWN_YISHUN  \\\n",
      "0               0              0               0               0            0   \n",
      "1               0              0               0               0            0   \n",
      "2               0              0               0               0            0   \n",
      "3               0              0               0               0            0   \n",
      "4               0              0               0               0            0   \n",
      "\n",
      "   FLAT_TYPE_3 ROOM  FLAT_TYPE_4 ROOM  FLAT_TYPE_5 ROOM  FLAT_TYPE_EXECUTIVE  \\\n",
      "0                 0                 1                 0                    0   \n",
      "1                 1                 0                 0                    0   \n",
      "2                 0                 1                 0                    0   \n",
      "3                 0                 1                 0                    0   \n",
      "4                 0                 1                 0                    0   \n",
      "\n",
      "   FLAT_TYPE_MULTI-GENERATION  \n",
      "0                           0  \n",
      "1                           0  \n",
      "2                           0  \n",
      "3                           0  \n",
      "4                           0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "‚úÖ train set numeric dtypes: int32    31\n",
      "int64     5\n",
      "Name: count, dtype: int64\n",
      "‚úÖ valid set numeric dtypes: int32    31\n",
      "int64     5\n",
      "Name: count, dtype: int64\n",
      "‚úÖ test set numeric dtypes: int32    31\n",
      "int64     5\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Data ready for training (train/valid/test) - 09:07:10\n",
      "0:\tlearn: 0.2997194\ttest: 0.2996630\tbest: 0.2996630 (0)\ttotal: 247ms\tremaining: 6m 34s\n",
      "100:\tlearn: 0.0822664\ttest: 0.0835716\tbest: 0.0835716 (100)\ttotal: 6.83s\tremaining: 1m 41s\n",
      "200:\tlearn: 0.0723052\ttest: 0.0742996\tbest: 0.0742996 (200)\ttotal: 12.8s\tremaining: 1m 29s\n",
      "300:\tlearn: 0.0676991\ttest: 0.0705753\tbest: 0.0705753 (300)\ttotal: 18.6s\tremaining: 1m 20s\n",
      "400:\tlearn: 0.0651198\ttest: 0.0687157\tbest: 0.0687157 (400)\ttotal: 25.8s\tremaining: 1m 17s\n",
      "500:\tlearn: 0.0632306\ttest: 0.0674761\tbest: 0.0674761 (500)\ttotal: 32.5s\tremaining: 1m 11s\n",
      "600:\tlearn: 0.0618553\ttest: 0.0666389\tbest: 0.0666389 (600)\ttotal: 39.2s\tremaining: 1m 5s\n",
      "700:\tlearn: 0.0607201\ttest: 0.0660093\tbest: 0.0660093 (700)\ttotal: 46.7s\tremaining: 59.9s\n",
      "800:\tlearn: 0.0596785\ttest: 0.0655017\tbest: 0.0655017 (800)\ttotal: 53s\tremaining: 52.9s\n",
      "900:\tlearn: 0.0587705\ttest: 0.0650945\tbest: 0.0650945 (900)\ttotal: 59.1s\tremaining: 45.8s\n",
      "1000:\tlearn: 0.0578953\ttest: 0.0647171\tbest: 0.0647171 (1000)\ttotal: 1m 5s\tremaining: 39.1s\n",
      "1100:\tlearn: 0.0571433\ttest: 0.0644375\tbest: 0.0644375 (1100)\ttotal: 1m 11s\tremaining: 32.4s\n",
      "1200:\tlearn: 0.0564081\ttest: 0.0641999\tbest: 0.0641999 (1200)\ttotal: 1m 17s\tremaining: 25.8s\n",
      "1300:\tlearn: 0.0558193\ttest: 0.0640057\tbest: 0.0640057 (1300)\ttotal: 1m 23s\tremaining: 19.2s\n",
      "1400:\tlearn: 0.0551733\ttest: 0.0637836\tbest: 0.0637836 (1400)\ttotal: 1m 29s\tremaining: 12.7s\n",
      "1500:\tlearn: 0.0546653\ttest: 0.0636677\tbest: 0.0636677 (1500)\ttotal: 1m 34s\tremaining: 6.24s\n",
      "1599:\tlearn: 0.0541814\ttest: 0.0635572\tbest: 0.0635572 (1599)\ttotal: 1m 42s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.0635572074\n",
      "bestIteration = 1599\n",
      "\n",
      "\n",
      "üìä CatBoost Training Results (Actual Price Scale)\n",
      "RMSE: 27,788.80 | MAE: 19,905.07 | R¬≤: 0.9712\n",
      "\n",
      "üìä CatBoost Validation Results (Actual Price Scale)\n",
      "RMSE: 33,502.85 | MAE: 23,652.43 | R¬≤: 0.9580\n",
      "\n",
      "üìä CatBoost Test Results (Actual Price Scale)\n",
      "RMSE: 33,742.34 | MAE: 23,694.59 | R¬≤: 0.9575\n",
      "‚úÖ Trained CatBoost model saved to 'catboost_model_valid_test.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üß† Train with different years and data size combination with best CatBoost parameters \n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# ========================================================\n",
    "# 1. Load data & Filter by year\n",
    "# ========================================================\n",
    "df = pd.read_csv('raw_data_main.csv')\n",
    "\n",
    "# --- Filter data for years 2020 to 2025 ---\n",
    "# df = df[(df['YEAR'] >= 2015) & (df['YEAR'] <= 2025)].copy()\n",
    "# The .copy() ensures we're working on a new DataFrame to avoid a SettingWithCopyWarning.\n",
    "\n",
    "# Exclude outliers (remove rows where IS_OUTLIERS = 1)\n",
    "df = df[df['IS_OUTLIERS'] != 1]\n",
    "\n",
    "# Create DATE_IDX (optional)\n",
    "df['DATE_IDX'] = df['YEAR'] * 12 + df['MONTH_NUM']\n",
    "\n",
    "# Log-transform target\n",
    "df['RESALE_PRICE'] = np.log1p(df['RESALE_PRICE'])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. Drop unwanted columns BEFORE preparing features\n",
    "# --------------------------------------------------------\n",
    "drop_cols = ['IS_OUTLIERS', 'STOREY_RANGE', 'PRICE_PER_SQM', 'MONTH_NUM','PRICE_TIER','SEASON','AGE_GROUP']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Define categorical variables to encode\n",
    "categorical_cols = ['TOWN', 'FLAT_TYPE']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "print(f\"üìå Applying one-hot encoding on: {categorical_cols}\")\n",
    "df = pd.get_dummies(df, columns=categorical_cols, dtype=int)\n",
    "print(\"‚úÖ One-hot encoding complete.\")\n",
    "print(\"üìå Encoded columns preview:\", df.columns.tolist()[:20])\n",
    "print(df.head())\n",
    "\n",
    "# Optional: sample smaller subset for quick experiments\n",
    "#df = df.sample(min(100000, len(df)), random_state=42)\n",
    "# Updated to handle datasets smaller than 100k\n",
    "\n",
    "# Create bin for stratified sampling\n",
    "df['price_bin'] = pd.qcut(df['RESALE_PRICE'], q=4, labels=False)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. Train / Validation / Test split\n",
    "# --------------------------------------------------------\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['price_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train, df_valid = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.25,\n",
    "    stratify=df_trainval['price_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Drop helper column used for stratification\n",
    "df_train = df_train.drop(columns=['price_bin'])\n",
    "df_valid = df_valid.drop(columns=['price_bin'])\n",
    "df_test = df_test.drop(columns=['price_bin'])\n",
    "\n",
    "# ========================================================\n",
    "# 4. Prepare features and target (no further dropping needed)\n",
    "# ========================================================\n",
    "X_train = df_train.drop(columns=['RESALE_PRICE'])\n",
    "y_train = df_train['RESALE_PRICE']\n",
    "\n",
    "X_valid = df_valid.drop(columns=['RESALE_PRICE'])\n",
    "y_valid = df_valid['RESALE_PRICE']\n",
    "\n",
    "X_test = df_test.drop(columns=['RESALE_PRICE'])\n",
    "y_test = df_test['RESALE_PRICE']\n",
    "\n",
    "# Ensure all numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_valid = X_valid.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Optional: sanity check\n",
    "for name, dfX in [(\"train\", X_train), (\"valid\", X_valid), (\"test\", X_test)]:\n",
    "    print(f\"‚úÖ {name} set numeric dtypes:\", dfX.dtypes.value_counts())\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(f\"‚úÖ Data ready for training (train/valid/test) - {timestamp}\")\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 5. Load best CatBoost parameters and train model\n",
    "# ========================================================\n",
    "'''\n",
    "def evaluate(model, X, y, label=\"\"):\n",
    "    \"\"\"Helper function to evaluate the model and print metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {label} Results\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "'''\n",
    "def evaluate_actual_scale(model, X, y_log, label=\"\"):\n",
    "    \"\"\"Evaluate model predictions in original price scale.\"\"\"\n",
    "    # Inverse transform\n",
    "    y_pred_log = model.predict(X)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_true = np.expm1(y_log)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nüìä {label} Results (Actual Price Scale)\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "    \n",
    "# 1. Load best parameters from JSON\n",
    "with open(\"best_catboost_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Remove R¬≤ score from params dict if it exists\n",
    "best_params.pop('r2', None)\n",
    "\n",
    "# 2. Train the model on X_train\n",
    "model = CatBoostRegressor(\n",
    "    verbose=100, # Reduced verbosity for cleaner output\n",
    "    random_state=42,\n",
    "    loss_function='RMSE',\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_valid, y_valid),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# 3. Predict on validation set & Evaluate model\n",
    "# evaluate(model, X_valid, y_valid, label=\"CatBoost Validation\")\n",
    "\n",
    "# Training set\n",
    "evaluate_actual_scale(model, X_train, y_train, label=\"CatBoost Training\")\n",
    "\n",
    "# Validation set\n",
    "evaluate_actual_scale(model, X_valid, y_valid, label=\"CatBoost Validation\")\n",
    "\n",
    "# Test set\n",
    "evaluate_actual_scale(model, X_test, y_test, label=\"CatBoost Test\")\n",
    "\n",
    "\n",
    "# 4. Save the trained model\n",
    "joblib.dump(model, \"catboost_model_valid_test.pkl\")\n",
    "print(\"‚úÖ Trained CatBoost model saved to 'catboost_model_valid_test.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Box-Cox Training Results (Actual Price Scale)\n",
      "RMSE: 38,975.50 | MAE: 26,126.16 | R¬≤: 0.9470\n",
      "\n",
      "üìä Box-Cox Validation Results (Actual Price Scale)\n",
      "RMSE: 45,499.71 | MAE: 30,309.41 | R¬≤: 0.9273\n",
      "\n",
      "üìä Box-Cox Test Results (Actual Price Scale)\n",
      "RMSE: 45,632.29 | MAE: 30,464.16 | R¬≤: 0.9276\n",
      "\n",
      "üìä Yeo-Johnson Training Results (Actual Price Scale)\n",
      "RMSE: 38,932.62 | MAE: 26,102.33 | R¬≤: 0.9471\n",
      "\n",
      "üìä Yeo-Johnson Validation Results (Actual Price Scale)\n",
      "RMSE: 45,452.76 | MAE: 30,271.14 | R¬≤: 0.9275\n",
      "\n",
      "üìä Yeo-Johnson Test Results (Actual Price Scale)\n",
      "RMSE: 45,579.65 | MAE: 30,435.70 | R¬≤: 0.9277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45579.653289440546, 30435.696462975953, 0.9277205101104038)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================================\n",
    "# üîÅ Compare Box-Cox vs Yeo-Johnson Target Transformations\n",
    "# ========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('raw_data_main.csv')\n",
    "\n",
    "# Create DATE_IDX\n",
    "df['DATE_IDX'] = df['YEAR'] * 12 + df['MONTH_NUM']\n",
    "\n",
    "# Drop unwanted columns\n",
    "drop_cols = ['IS_OUTLIERS', 'STOREY_RANGE', 'PRICE_PER_SQM', 'YEAR', 'MONTH_NUM','PRICE_TIER','SEASON','AGE_GROUP']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_cols = ['TOWN', 'FLAT_TYPE']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "df = pd.get_dummies(df, columns=categorical_cols, dtype=int)\n",
    "\n",
    "# Create stratification bin\n",
    "df['price_bin'] = pd.qcut(df['RESALE_PRICE'], q=4, labels=False)\n",
    "\n",
    "# Train/valid/test split\n",
    "df_trainval, df_test = train_test_split(df, test_size=0.2, stratify=df['price_bin'], random_state=42)\n",
    "df_train, df_valid = train_test_split(df_trainval, test_size=0.25, stratify=df_trainval['price_bin'], random_state=42)\n",
    "\n",
    "# Drop bin column\n",
    "for d in [df_train, df_valid, df_test]:\n",
    "    d.drop(columns=['price_bin'], inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "def prepare_X_y(df):\n",
    "    X = df.drop(columns=['RESALE_PRICE'])\n",
    "    y = df['RESALE_PRICE']\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train_raw = prepare_X_y(df_train)\n",
    "X_valid, y_valid_raw = prepare_X_y(df_valid)\n",
    "X_test, y_test_raw = prepare_X_y(df_test)\n",
    "\n",
    "# Apply PowerTransformers\n",
    "pt_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
    "pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "\n",
    "# Box-Cox requires strictly positive values\n",
    "y_train_bc = pt_boxcox.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()\n",
    "y_valid_bc = pt_boxcox.transform(y_valid_raw.values.reshape(-1, 1)).flatten()\n",
    "y_test_bc  = pt_boxcox.transform(y_test_raw.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Yeo-Johnson works with zero or negative values\n",
    "y_train_yj = pt_yeojohnson.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()\n",
    "y_valid_yj = pt_yeojohnson.transform(y_valid_raw.values.reshape(-1, 1)).flatten()\n",
    "y_test_yj  = pt_yeojohnson.transform(y_test_raw.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Load best CatBoost parameters\n",
    "with open(\"best_catboost_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "best_params.pop('r2', None)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_inverse(model, X, y_transformed, transformer, label=\"\"):\n",
    "    y_pred_transformed = model.predict(X)\n",
    "    y_pred = transformer.inverse_transform(y_pred_transformed.reshape(-1, 1)).flatten()\n",
    "    y_true = transformer.inverse_transform(y_transformed.reshape(-1, 1)).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\nüìä {label} Results (Actual Price Scale)\")\n",
    "    print(f\"RMSE: {rmse:,.2f} | MAE: {mae:,.2f} | R¬≤: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Train and evaluate Box-Cox model\n",
    "model_bc = CatBoostRegressor(verbose=0, random_state=42, loss_function='RMSE', **best_params)\n",
    "model_bc.fit(X_train, y_train_bc, eval_set=(X_valid, y_valid_bc), early_stopping_rounds=50)\n",
    "\n",
    "evaluate_inverse(model_bc, X_train, y_train_bc, pt_boxcox, label=\"Box-Cox Training\")\n",
    "evaluate_inverse(model_bc, X_valid, y_valid_bc, pt_boxcox, label=\"Box-Cox Validation\")\n",
    "evaluate_inverse(model_bc, X_test,  y_test_bc,  pt_boxcox, label=\"Box-Cox Test\")\n",
    "\n",
    "# Train and evaluate Yeo-Johnson model\n",
    "model_yj = CatBoostRegressor(verbose=0, random_state=42, loss_function='RMSE', **best_params)\n",
    "model_yj.fit(X_train, y_train_yj, eval_set=(X_valid, y_valid_yj), early_stopping_rounds=50)\n",
    "\n",
    "evaluate_inverse(model_yj, X_train, y_train_yj, pt_yeojohnson, label=\"Yeo-Johnson Training\")\n",
    "evaluate_inverse(model_yj, X_valid, y_valid_yj, pt_yeojohnson, label=\"Yeo-Johnson Validation\")\n",
    "evaluate_inverse(model_yj, X_test,  y_test_yj,  pt_yeojohnson, label=\"Yeo-Johnson Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
